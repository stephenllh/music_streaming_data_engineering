{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d192f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, from_json, year, month, hour, dayofmonth\n",
    "\n",
    "\n",
    "def create_session(app_name, master=\"yarn\"):\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)  # .master(master)\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "\n",
    "def read_stream(spark, kafka_ip_address, kafka_port, kafka_topic):\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", f\"{kafka_ip_address}:{kafka_port}\")\n",
    "        .option(\"failOnDataLoss\", False)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .option(\"subscribe\", kafka_topic)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "\n",
    "def process_stream(stream, stream_schema, topic_name):\n",
    "    stream = (\n",
    "        stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "        .select(from_json(col(\"value\"), stream_schema).alias(\"data\"))\n",
    "        .select(\"data.*\")\n",
    "    )\n",
    "\n",
    "    stream = (\n",
    "        stream.withColumn(\"ts\", (col(\"ts\") / 1000).cast(\"timestamp\"))\n",
    "        .withColumn(\"year\", year(col(\"ts\")))\n",
    "        .withColumn(\"month\", month(col(\"ts\")))\n",
    "        .withColumn(\"hour\", hour(col(\"ts\")))\n",
    "        .withColumn(\"day\", dayofmonth(col(\"ts\")))\n",
    "    )\n",
    "\n",
    "    if topic_name in [\"listen_events\", \"page_view_events\"]:\n",
    "        stream = (\n",
    "            stream.withColumn(\"song\", string_decode(\"song\"))\n",
    "            .withColumn(\"artist\", string_decode(\"artist\"))\n",
    "        )\n",
    "\n",
    "    return stream\n",
    "\n",
    "\n",
    "def write_stream(stream, file_format, storage_path, checkpoint_path, trigger, output_mode):\n",
    "    return (\n",
    "        stream.writeStream\n",
    "        .format(file_format)\n",
    "        #.partitionBy(\"month\", \"day\", \"hour\")\n",
    "        .option(\"path\", storage_path)\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .trigger(processingTime=trigger)\n",
    "        .outputMode(output_mode)\n",
    "    )\n",
    "\n",
    "\n",
    "def process_and_write_stream(\n",
    "    stream, stream_schema, topic_name, file_format, storage_path, checkpoint_path, trigger, output_mode\n",
    "):\n",
    "    stream = process_stream(stream, stream_schema, topic_name)\n",
    "    print(\"done process!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    stream = write_stream(stream, file_format, storage_path, checkpoint_path, trigger, output_mode)\n",
    "    stream.start()\n",
    "\n",
    "\n",
    "@udf\n",
    "def string_decode(s, encoding=\"utf-8\"):\n",
    "    if s:\n",
    "        return (\n",
    "            s.encode(\"latin1\")\n",
    "            .decode(\"unicode-escape\")\n",
    "            .encode('latin1')\n",
    "            .decode(encoding)\n",
    "            .strip('\"')\n",
    "        )\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a29170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from schema import schema\n",
    "from utils import create_session, read_stream, process_and_write_stream\n",
    "\n",
    "\"\"\"\n",
    "Run the script using the following command:\n",
    "\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 stream_all_events.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "KAFKA_IP_ADDRESS = os.getenv(\"KAFKA_ADDRESS\", \"localhost\")\n",
    "KAFKA_PORT = 9092\n",
    "STORAGE_PATH = \"../output\"\n",
    "TRIGGER_PERIOD = \"10 seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43779c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_session(\"Eventsim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d821eacf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19589/2630833025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKAFKA_PORT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"auth_events\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_19589/3672117102.py\u001b[0m in \u001b[0;36mread_stream\u001b[0;34m(spark, kafka_ip_address, kafka_port, kafka_topic)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkafka_ip_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkafka_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkafka_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     return (\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{kafka_ip_address}:{kafka_port}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".;"
     ]
    }
   ],
   "source": [
    "stream = read_stream(spark, \"localhost\", KAFKA_PORT, \"auth_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7399cbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/stephenllh/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20152c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"ADFASDFASDFASDFASDFASDFASDF\", streams[0])\n",
    "\n",
    "for stream, kafka_topic in zip(streams, kafka_topics):\n",
    "    process_and_write_stream(\n",
    "        stream,\n",
    "        stream_schema=schema[kafka_topic],\n",
    "        topic_name=kafka_topic,\n",
    "        file_format=\"parquet\",\n",
    "        storage_path=f\"{STORAGE_PATH}/{kafka_topic}\",\n",
    "        checkpoint_path=f\"{STORAGE_PATH}/checkpoint/{kafka_topic}\",\n",
    "        trigger=TRIGGER_PERIOD,\n",
    "        output_mode=\"append\",\n",
    "    )\n",
    "\n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
